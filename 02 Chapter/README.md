This chapter begins with a description of the basic components of an MDP, namely the planning horizon, decision epochs, states, actions, transition probabilities, and rewards. Next, it describes derived objectives including decision rules, policies, derived stochastic processes, and reward processes. Formal definitions of the expected total reward, expected discounted reward, and long-run average reward criteria are presented for finite and infinite horizon settings. A one-period model is presented, which serves as the fundamental building block of a Markov decision process. It is used to describe key concepts that will be presented in more generality later on, including value functions, state-action value functions, optimal values, and optimal policies. Deterministic and randomized policies are distinguished and a one-period problem illustrates the fundamental result that deterministic policies typically suffice. Finally, a two-state MDP model is presented and analyzed in depth. This is an important example that will show up throughout the book to illustrate new concepts as they are presented.

ERRATA:
Updated on August 28,2025 to correct typo in Figure 2.5.

