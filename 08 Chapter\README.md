Chapter 8 studies partially observable Markov decision processes. In a POMDP, the decision maker cannot observe the system state but instead receives a signal that provides noisy information about the hidden state. The key idea is to leverage Bayes' theorem to update a prior probability distribution over the hidden states using the action taken and signal observed to obtain a posterior distribution. These probabilities distributions are referred to as belief states, which are equivalent to states in a fully observable MDP, thus transforming a finite-state POMDP into an equivalent uncountable-state MDP. The POMDP value function is shown to be piecewise linear convex, which leads to an exact but complex algorithm for solving finite horizon POMDPs. Approximate solution methods are provided. A brief discussion on infinite horizon POMDPs is included. Formulations of the following applications are presented: preventive maintenance and inspection, Bayesian decision problems, multi-armed bandits, breast cancer screening, and robotic control. 
