This chapter studies infinite horizon Markov decision processes under the expected total reward criterion. Without discounting, care must be taken to ensure that the relevant quantities such as the policy value function is well-defined and finite. Finiteness is achieved by restricting attention to models that reach a set of zero-reward absorbing states in a finite expected number of transitions by some or all policies. Four types of expected total reward models are covered: transient, stochastic shortest path, positive and negative. The Bellman equation is provided. Value iteration, policy iteration, modified policy iteration and linear programming approaches to solving the model are presented. A brief discussion on structured policies is included. The chapter concludes with two case studies: Gridworld navigation and optimal stopping. 
