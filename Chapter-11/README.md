Chapter 11 combines function approximation with  simulation. Whereas value function and state-action value function approximation generalize methods from the previous chapter, this chapter introduces policy approximation, namely, approximating the action-choice probability distribution of a randomized policy. Value function approximation using Monte Carlo and temporal differencing methods is described. Optimization algorithms classified as value-based algorithms (Q-learning and Q-policy iteration) and policy-based algorithms (policy gradient and actor-critic) are presented. The chapter concludes with brief remarks on additional topics including simultaneous learning and control, experience replay, deep learning, importance sampling, Monte Carlo tree search and partially observable models. 
