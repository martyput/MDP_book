This chapter describes the three parts of the book, namely MDP fundamentals, classical MDP models, and the foundations of reinforcement learning. It presents perspectives on model-based and model-free approaches to sequential decision making under uncertainty. For both Markov decision processes and reinforcement learning, it provides a brief history and a summary of applications. Book objectives are presented. Finally, a detailed comparison is made with Puterman's earlier Markov decision process book, emphasizing that this book is not a revision, but an updated and more accessible alternative.
