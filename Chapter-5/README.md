This chapter concerns infinite horizon Markov decision processes under the expected discounted reward criterion. It establishes the optimality of stationary deterministic policies within the class of all policies and the existence and uniqueness of the solution to the Bellman equation. It covers three classes of iterative algorithms for solving these models, namely value iteration, policy iteration and modified policy iteration. In each case, it describes the algorithm and its variants, provides stopping rules, proves convergence and illustrates their application examples. It also describes the linear programming formulation of this MDP model, focusing on key conceptual equivalences. Interpretability and optimality of structured policies are discussed. It concludes with two in-depth case studies: queuing service rate control and clinical decision making.
