Chapter 4 describes the finite horizon model, one in which where the Markov decision process ends at a fixed future stage that is known at the start of the planning horizon. It introduces the concepts of the value of a policy (policy value function), an optimal policy, the optimal value function, the state-action value function, the principle of optimality, the optimality (Bellman) equation, and backwards induction. These concepts are formally defined and illustrated through numerous examples. Interpretability and optimality of structured policies are discussed. The intuition developed in this chapter is critical to understanding infinite horizon MDPs.
